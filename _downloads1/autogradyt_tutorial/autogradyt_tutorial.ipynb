{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaredmccain-ux/pytorch-tutorial-learning-notes/blob/main/_downloads1/autogradyt_tutorial/autogradyt_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNpQpX81sWM0"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://docs.pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBOS3YjBsWM2"
      },
      "source": [
        "[Introduction](introyt1_tutorial.html) \\|\\|\n",
        "[Tensors](tensors_deeper_tutorial.html) \\|\\| **Autograd** \\|\\| [Building\n",
        "Models](modelsyt_tutorial.html) \\|\\| [TensorBoard\n",
        "Support](tensorboardyt_tutorial.html) \\|\\| [Training\n",
        "Models](trainingyt.html) \\|\\| [Model Understanding](captumyt.html)\n",
        "\n",
        "The Fundamentals of Autograd\n",
        "============================\n",
        "\n",
        "Follow along with the video below or on\n",
        "[youtube](https://www.youtube.com/watch?v=M0fX15_-xrY).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4c96ff48",
        "outputId": "33107f6e-adeb-4085-863a-2859e773c114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"margin-top:10px; margin-bottom:10px;\">\n",
              "  <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/M0fX15_-xrY\" frameborder=\"0\" allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Run this cell to load the video\n",
        "from IPython.display import display, HTML\n",
        "html_code = \"\"\"\n",
        "<div style=\"margin-top:10px; margin-bottom:10px;\">\n",
        "  <iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/M0fX15_-xrY\" frameborder=\"0\" allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
        "</div>\n",
        "\"\"\"\n",
        "display(HTML(html_code))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03a101ee"
      },
      "source": [
        "\n",
        "\n",
        "PyTorch's *Autograd* feature is part of what make PyTorch flexible and\n",
        "fast for building machine learning projects. It allows for the rapid and\n",
        "easy computation of multiple partial derivatives (also referred to as\n",
        "*gradients)* over a complex computation. This operation is central to\n",
        "backpropagation-based neural network learning.\n",
        "\n",
        "The power of autograd comes from the fact that it traces your\n",
        "computation dynamically *at runtime,* meaning that if your model has\n",
        "decision branches, or loops whose lengths are not known until runtime,\n",
        "the computation will still be traced correctly, and you'll get correct\n",
        "gradients to drive learning. This, combined with the fact that your\n",
        "models are built in Python, offers far more flexibility than frameworks\n",
        "that rely on static analysis of a more rigidly-structured model for\n",
        "computing gradients.\n",
        "\n",
        "What Do We Need Autograd For?\n",
        "-----------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbOKN1-osWM4"
      },
      "source": [
        "A machine learning model is a *function*, with inputs and outputs. For\n",
        "this discussion, we'll treat the inputs as an *i*-dimensional vector\n",
        "$\\vec{x}$, with elements $x_{i}$. We can then express the model, *M*, as\n",
        "a vector-valued function of the input: $\\vec{y} =\n",
        "\\vec{M}(\\vec{x})$. (We treat the value of M's output as a vector because\n",
        "in general, a model may have any number of outputs.)\n",
        "\n",
        "Since we'll mostly be discussing autograd in the context of training,\n",
        "our output of interest will be the model's loss. The *loss function*\n",
        "L($\\vec{y}$) = L($\\vec{M}$($\\vec{x}$)) is a single-valued scalar\n",
        "function of the model's output. This function expresses how far off our\n",
        "model's prediction was from a particular input's *ideal* output. *Note:\n",
        "After this point, we will often omit the vector sign where it should be\n",
        "contextually clear - e.g.,* $y$ instead of $\\vec y$.\n",
        "\n",
        "In training a model, we want to minimize the loss. In the idealized case\n",
        "of a perfect model, that means adjusting its learning weights - that is,\n",
        "the adjustable parameters of the function - such that loss is zero for\n",
        "all inputs. In the real world, it means an iterative process of nudging\n",
        "the learning weights until we see that we get a tolerable loss for a\n",
        "wide variety of inputs.\n",
        "\n",
        "How do we decide how far and in which direction to nudge the weights? We\n",
        "want to *minimize* the loss, which means making its first derivative\n",
        "with respect to the input equal to 0:\n",
        "$\\frac{\\partial L}{\\partial x} = 0$.\n",
        "\n",
        "Recall, though, that the loss is not *directly* derived from the input,\n",
        "but a function of the model's output (which is a function of the input\n",
        "directly), $\\frac{\\partial L}{\\partial x}$ =\n",
        "$\\frac{\\partial {L({\\vec y})}}{\\partial x}$. By the chain rule of\n",
        "differential calculus, we have\n",
        "$\\frac{\\partial {L({\\vec y})}}{\\partial x}$ =\n",
        "$\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial x}$ =\n",
        "$\\frac{\\partial L}{\\partial y}\\frac{\\partial M(x)}{\\partial x}$.\n",
        "\n",
        "$\\frac{\\partial M(x)}{\\partial x}$ is where things get complex. The\n",
        "partial derivatives of the model's outputs with respect to its inputs,\n",
        "if we were to expand the expression using the chain rule again, would\n",
        "involve many local partial derivatives over every multiplied learning\n",
        "weight, every activation function, and every other mathematical\n",
        "transformation in the model. The full expression for each such partial\n",
        "derivative is the sum of the products of the local gradient of *every\n",
        "possible path* through the computation graph that ends with the variable\n",
        "whose gradient we are trying to measure.\n",
        "\n",
        "In particular, the gradients over the learning weights are of interest\n",
        "to us - they tell us *what direction to change each weight* to get the\n",
        "loss function closer to zero.\n",
        "\n",
        "Since the number of such local derivatives (each corresponding to a\n",
        "separate path through the model's computation graph) will tend to go up\n",
        "exponentially with the depth of a neural network, so does the complexity\n",
        "in computing them. This is where autograd comes in: It tracks the\n",
        "history of every computation. Every computed tensor in your PyTorch\n",
        "model carries a history of its input tensors and the function used to\n",
        "create it. Combined with the fact that PyTorch functions meant to act on\n",
        "tensors each have a built-in implementation for computing their own\n",
        "derivatives, this greatly speeds the computation of the local\n",
        "derivatives needed for learning.\n",
        "\n",
        "A Simple Example\n",
        "================\n",
        "\n",
        "That was a lot of theory - but what does it look like to use autograd in\n",
        "practice?\n",
        "\n",
        "Let's start with a straightforward example. First, we'll do some imports\n",
        "to let us graph our results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2Z17nH2BsWM4"
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline\n",
        "\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aR6htF7sWM5"
      },
      "source": [
        "Next, we'll create an input tensor full of evenly spaced values on the\n",
        "interval $[0, 2{\\pi}]$, and specify `requires_grad=True`. (Like most\n",
        "functions that create tensors, `torch.linspace()` accepts an optional\n",
        "`requires_grad` option.) Setting this flag means that in every\n",
        "computation that follows, autograd will be accumulating the history of\n",
        "the computation in the output tensors of that computation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7J3bvXOjsWM5",
        "outputId": "9f70b1b4-d6a8-4700-a067-657c53bb619b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0000, 0.2618, 0.5236, 0.7854, 1.0472, 1.3090, 1.5708, 1.8326, 2.0944,\n",
            "        2.3562, 2.6180, 2.8798, 3.1416, 3.4034, 3.6652, 3.9270, 4.1888, 4.4506,\n",
            "        4.7124, 4.9742, 5.2360, 5.4978, 5.7596, 6.0214, 6.2832],\n",
            "       requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **解释linsapce:**\n",
        "这段代码使用了 PyTorch 库中的 `torch.linspace` 函数。它的主要作用是**生成一个包含等间距数值的一维张量（Tensor）**，并且开启了自动求导功能。\n",
        "\n",
        "### 1. 代码具体含义 breakdown\n",
        "\n",
        "`torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)`\n",
        "\n",
        "* **`0.` (start)**: 数列的**起始值**。\n",
        "* **`2. * math.pi` (end)**: 数列的**结束值**（即 ，约为 6.283）。\n",
        "* **`steps=25`**: 数列中包含的**元素个数**。它会在起始值和结束值之间均匀地切分出 25 个点（包含首尾）。\n",
        "* **`requires_grad=True`**: 这是一个关键参数。它告诉 PyTorch **需要追踪在这个张量上进行的所有计算操作**。\n",
        "* 这意味着这个张量被视为计算图中的一个“叶子节点”。\n",
        "* 在后续的反向传播（`backward()`）中，PyTorch 会计算关于这个张量的梯度，并将其存储在该张量的 `.grad` 属性中。\n"
      ],
      "metadata": {
        "id": "Udwrq9J3xMX4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flvt6gGisWM5"
      },
      "source": [
        "Next, we'll perform a computation, and plot its output in terms of its\n",
        "inputs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1-nUah0XsWM6",
        "outputId": "b41544c9-e034-4ad1-aa9b-2c0d0c2f734c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7802fa788a40>]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "b = torch.sin(a)\n",
        "plt.plot(a.detach(), b.detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWpilODVsWM6"
      },
      "source": [
        "Let's have a closer look at the tensor `b`. When we print it, we see an\n",
        "indicator that it is tracking its computation history:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "luHGUWv3sWM6",
        "outputId": "33abcfd0-caf7-4772-e81a-f5119c55f121",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0000e+00,  2.5882e-01,  5.0000e-01,  7.0711e-01,  8.6603e-01,\n",
            "         9.6593e-01,  1.0000e+00,  9.6593e-01,  8.6603e-01,  7.0711e-01,\n",
            "         5.0000e-01,  2.5882e-01, -8.7423e-08, -2.5882e-01, -5.0000e-01,\n",
            "        -7.0711e-01, -8.6603e-01, -9.6593e-01, -1.0000e+00, -9.6593e-01,\n",
            "        -8.6603e-01, -7.0711e-01, -5.0000e-01, -2.5882e-01,  1.7485e-07],\n",
            "       grad_fn=<SinBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTKh80RKsWM6"
      },
      "source": [
        "This `grad_fn` gives us a hint that when we execute the backpropagation\n",
        "step and compute gradients, we'll need to compute the derivative of\n",
        "$\\sin(x)$ for all this tensor's inputs.\n",
        "\n",
        "Let's perform some more computations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ESNIPvTGsWM6",
        "outputId": "b4208e37-ed3a-4ff2-8c16-9aeb0d15b868",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0000e+00,  5.1764e-01,  1.0000e+00,  1.4142e+00,  1.7321e+00,\n",
            "         1.9319e+00,  2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,\n",
            "         1.0000e+00,  5.1764e-01, -1.7485e-07, -5.1764e-01, -1.0000e+00,\n",
            "        -1.4142e+00, -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00,\n",
            "        -1.7321e+00, -1.4142e+00, -1.0000e+00, -5.1764e-01,  3.4969e-07],\n",
            "       grad_fn=<MulBackward0>)\n",
            "tensor([ 1.0000e+00,  1.5176e+00,  2.0000e+00,  2.4142e+00,  2.7321e+00,\n",
            "         2.9319e+00,  3.0000e+00,  2.9319e+00,  2.7321e+00,  2.4142e+00,\n",
            "         2.0000e+00,  1.5176e+00,  1.0000e+00,  4.8236e-01, -3.5763e-07,\n",
            "        -4.1421e-01, -7.3205e-01, -9.3185e-01, -1.0000e+00, -9.3185e-01,\n",
            "        -7.3205e-01, -4.1421e-01,  4.7684e-07,  4.8236e-01,  1.0000e+00],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "c = 2 * b\n",
        "print(c)\n",
        "\n",
        "d = c + 1\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UOTTP7PsWM7"
      },
      "source": [
        "Finally, let's compute a single-element output. When you call\n",
        "`.backward()` on a tensor with no arguments, it expects the calling\n",
        "tensor to contain only a single element, as is the case when computing a\n",
        "loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kY9DgGZwsWM7",
        "outputId": "3bf886fe-26e7-47ba-eee4-15f4621e5863",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(25., grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "out = d.sum()\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHhbh3ZpsWM7"
      },
      "source": [
        "Each `grad_fn` stored with our tensors allows you to walk the\n",
        "computation all the way back to its inputs with its `next_functions`\n",
        "property. We can see below that drilling down on this property on `d`\n",
        "shows us the gradient functions for all the prior tensors. Note that\n",
        "`a.grad_fn` is reported as `None`, indicating that this was an input to\n",
        "the function with no history of its own.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y7MRsi8QsWM7",
        "outputId": "bcda551e-839f-45de-dabc-170bfe12f52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d:\n",
            "<AddBackward0 object at 0x7802fa7121a0>\n",
            "((<MulBackward0 object at 0x7802fa7a4c40>, 0), (None, 0))\n",
            "((<SinBackward0 object at 0x7802fa7a4c40>, 0), (None, 0))\n",
            "((<AccumulateGrad object at 0x7802fa7121a0>, 0),)\n",
            "()\n",
            "\n",
            "c:\n",
            "<MulBackward0 object at 0x7802fa7a4c40>\n",
            "\n",
            "b:\n",
            "<SinBackward0 object at 0x7802fa7a4c40>\n",
            "\n",
            "a:\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print('d:')\n",
        "print(d.grad_fn)\n",
        "print(d.grad_fn.next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
        "print('\\nc:')\n",
        "print(c.grad_fn)\n",
        "print('\\nb:')\n",
        "print(b.grad_fn)\n",
        "print('\\na:')\n",
        "print(a.grad_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS41-PgxsWM7"
      },
      "source": [
        "With all this machinery in place, how do we get derivatives out? You\n",
        "call the `backward()` method on the output, and check the input's `grad`\n",
        "property to inspect the gradients:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "umNboXrysWM7",
        "outputId": "194c37e0-d383-4d7d-e729-fbb96a2a9534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,  1.0000e+00,\n",
            "         5.1764e-01, -8.7423e-08, -5.1764e-01, -1.0000e+00, -1.4142e+00,\n",
            "        -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00, -1.7321e+00,\n",
            "        -1.4142e+00, -1.0000e+00, -5.1764e-01,  2.3850e-08,  5.1764e-01,\n",
            "         1.0000e+00,  1.4142e+00,  1.7321e+00,  1.9319e+00,  2.0000e+00])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7802fa63e270>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "out.backward()\n",
        "print(a.grad)\n",
        "plt.plot(a.detach(), a.grad.detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATlNz3X7sWM7"
      },
      "source": [
        "Recall the computation steps we took to get here:\n",
        "\n",
        "``` {.python}\n",
        "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
        "b = torch.sin(a)\n",
        "c = 2 * b\n",
        "d = c + 1\n",
        "out = d.sum()\n",
        "```\n",
        "\n",
        "Adding a constant, as we did to compute `d`, does not change the\n",
        "derivative. That leaves $c = 2 * b = 2 * \\sin(a)$, the derivative of\n",
        "which should be $2 * \\cos(a)$. Looking at the graph above, that's just\n",
        "what we see.\n",
        "\n",
        "Be aware that only *leaf nodes* of the computation have their gradients\n",
        "computed. If you tried, for example, `print(c.grad)` you'd get back\n",
        "`None`. In this simple example, only the input is a leaf node, so only\n",
        "it has gradients computed.\n",
        "这是一个非常经典的 PyTorch 概念问题，涉及**计算图（Computational Graph）和内存优化机制**。\n",
        "\n",
        "简单来说，那段 Note 的意思是：**为了节省内存，PyTorch 默认只会保留“源头”（即叶子节点）的梯度。中间过程产生的变量（如 `c`），算完梯度传给前面的人后，它自己的梯度就被扔掉了。**\n",
        "\n",
        "下面为你详细拆解“叶子节点”和“为什么 c 显示无梯度”。\n",
        "\n",
        "---\n",
        "\n",
        "### 1. 什么是 Leaf Node（叶子节点）？\n",
        "\n",
        "在 PyTorch 的计算图中，节点分为两类：**用户创建的节点** 和 **运算产生的节点**。\n",
        "\n",
        "* **Leaf Node (叶子节点)**:\n",
        "* **定义**: 由**用户直接创建**的 Tensor，而不是通过某种运算（如加减乘除）生成的。\n",
        "* **特征**: 它是整个计算链路的“源头”或“输入”。\n",
        "* **在你的代码中**:\n",
        "* `a`: 是你通过 `torch.linspace` 直接创建的。**它是叶子节点**。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Non-Leaf Node (非叶子节点 / 中间节点)**:\n",
        "* **定义**: 通过对其他 Tensor 进行运算**生成**的 Tensor。\n",
        "* **在你的代码中**:\n",
        "* `b`: 是由 `sin(a)` 算出来的。**不是叶子节点**。\n",
        "* `c`: 是由 `2 * b` 算出来的。**不是叶子节点**。\n",
        "* `d`: 是由 `c + 1` 算出来的。**不是叶子节点**。\n",
        "* `out`: 是由 `d.sum()` 算出来的。**不是叶子节点**。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> **判断方法**: 你可以打印 `tensor.is_leaf` 属性来查看。\n",
        "> `print(a.is_leaf)` -> `True`\n",
        "> `print(c.is_leaf)` -> `False`\n",
        "\n",
        "---\n",
        "\n",
        "### 2. 为什么 `c.grad` 是 None？\n",
        "\n",
        "当你执行 `out.backward()` 时，PyTorch 开始从 `out` 往回跑（反向传播），计算梯度。\n",
        "\n",
        "计算过程其实是这样的链条：\n",
        "\n",
        "1. 计算 `out` 对 `d` 的梯度  传给 `d`。\n",
        "2. 计算 `d` 对 `c` 的梯度  传给 `c`。\n",
        "3. 计算 `c` 对 `b` 的梯度  传给 `b`。\n",
        "4. 计算 `b` 对 `a` 的梯度  存入 `a.grad`。\n",
        "\n",
        "**关键点来了：**\n",
        "对于深度学习模型（可能有成百上千层），中间变量（如这里的 `b`, `c`, `d`）可能有数百万个参数。如果我们把这些中间变量的梯度全部保存下来，显存/内存瞬间就会爆炸。\n",
        "\n",
        "因此，PyTorch 采用了一种**“用完即弃”**的策略：\n",
        "\n",
        "* 只要该节点**不是叶子节点**（is_leaf=False），PyTorch 会在计算完它对前一层的贡献后，**立即释放**它所占用的梯度内存。\n",
        "* 只有**叶子节点**（`a`），因为它是我们需要更新的参数（或想要分析的输入），其梯度（`a.grad`）才会被保留。\n",
        "\n",
        "所以，虽然数学上 `c` 肯定有梯度，但程序运行完 `backward()` 后，`c.grad` 已经被清理掉了，所以你打印出来是 `None`。\n",
        "\n",
        "---\n",
        "\n",
        "### 3. 如果我非要看 `c` 的梯度怎么办？\n",
        "\n",
        "如果你在调试，非常想看中间变量 `c` 的梯度，PyTorch 提供了 `retain_grad()` 方法。它告诉 PyTorch：“这个中间节点的梯度别扔，给我留着。”\n",
        "\n",
        "\n",
        "\n",
        "Autograd in Training\n",
        "====================\n",
        "\n",
        "We've had a brief look at how autograd works, but how does it look when\n",
        "it's used for its intended purpose? Let's define a small model and\n",
        "examine how it changes after a single training batch. First, define a\n",
        "few constants, our model, and some stand-ins for inputs and outputs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P-zgNGStsWM7"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "DIM_IN = 1000\n",
        "HIDDEN_SIZE = 100\n",
        "DIM_OUT = 10\n",
        "\n",
        "class TinyModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TinyModel, self).__init__()\n",
        "\n",
        "        self.layer1 = torch.nn.Linear(DIM_IN, HIDDEN_SIZE)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.layer2 = torch.nn.Linear(HIDDEN_SIZE, DIM_OUT)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)\n",
        "ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)\n",
        "\n",
        "model = TinyModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8192NYbssWM8"
      },
      "source": [
        "One thing you might notice is that we never specify `requires_grad=True`\n",
        "for the model's layers. Within a subclass of `torch.nn.Module`, it's\n",
        "assumed that we want to track gradients on the layers' weights for\n",
        "learning.\n",
        "\n",
        "If we look at the layers of the model, we can examine the values of the\n",
        "weights, and verify that no gradients have been computed yet:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "S8TyZCw8sWM8",
        "outputId": "5fe8dd28-65b9-4b4c-ae4f-35130d0b46eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0762, -0.0943, -0.0259, -0.0456, -0.0546, -0.0482, -0.0620, -0.0919,\n",
            "         0.0231, -0.0499], grad_fn=<SliceBackward0>)\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(model.layer2.weight[0][0:10]) # just a small slice\n",
        "print(model.layer2.weight.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IwiSRMKsWM8"
      },
      "source": [
        "Let's see how this changes when we run through one training batch. For a\n",
        "loss function, we'll just use the square of the Euclidean distance\n",
        "between our `prediction` and the `ideal_output`, and we'll use a basic\n",
        "stochastic gradient descent optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "us_ZkvELsWM8",
        "outputId": "1ae88409-bdb3-40ae-fefb-e43ee48e6a21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(195.3370, grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "prediction = model(some_input)\n",
        "\n",
        "loss = (ideal_output - prediction).pow(2).sum()\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2EHYB7CsWM8"
      },
      "source": [
        "Now, let's call `loss.backward()` and see what happens:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MA0Mdf8HsWM8",
        "outputId": "69981dc8-35d7-407e-d00b-400383ff67bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0762, -0.0943, -0.0259, -0.0456, -0.0546, -0.0482, -0.0620, -0.0919,\n",
            "         0.0231, -0.0499], grad_fn=<SliceBackward0>)\n",
            "tensor([ 2.0410, -0.7253,  7.1410,  1.3626, -4.3957, -0.5833,  2.7548,  2.7618,\n",
            "        -5.4457,  1.9793])\n"
          ]
        }
      ],
      "source": [
        "loss.backward()\n",
        "print(model.layer2.weight[0][0:10])\n",
        "print(model.layer2.weight.grad[0][0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH6gyLPNsWM8"
      },
      "source": [
        "We can see that the gradients have been computed for each learning\n",
        "weight, but the weights remain unchanged, because we haven't run the\n",
        "optimizer yet. The optimizer is responsible for updating model weights\n",
        "based on the computed gradients.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D-etYSLOsWM8",
        "outputId": "34ff014b-3dc1-4335-c5bf-ec72bc125974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0742, -0.0936, -0.0330, -0.0470, -0.0503, -0.0476, -0.0648, -0.0946,\n",
            "         0.0285, -0.0519], grad_fn=<SliceBackward0>)\n",
            "tensor([ 2.0410, -0.7253,  7.1410,  1.3626, -4.3957, -0.5833,  2.7548,  2.7618,\n",
            "        -5.4457,  1.9793])\n"
          ]
        }
      ],
      "source": [
        "optimizer.step()\n",
        "print(model.layer2.weight[0][0:10])\n",
        "print(model.layer2.weight.grad[0][0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTVIKpMmsWM8"
      },
      "source": [
        "You should see that `layer2`'s weights have changed.\n",
        "\n",
        "One important thing about the process: After calling `optimizer.step()`,\n",
        "you need to call `optimizer.zero_grad()`, or else every time you run\n",
        "`loss.backward()`, the gradients on the learning weights will\n",
        "accumulate:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Duatvxa-sWM8",
        "outputId": "ad968a72-97ef-47f4-954e-97105d29f054",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.0410, -0.7253,  7.1410,  1.3626, -4.3957, -0.5833,  2.7548,  2.7618,\n",
            "        -5.4457,  1.9793])\n",
            "tensor([-14.3042,   0.1162,  32.9913,   5.2410, -10.3861,   3.9332,  18.4499,\n",
            "         15.2335, -30.4767,  17.9407])\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ],
      "source": [
        "print(model.layer2.weight.grad[0][0:10])\n",
        "\n",
        "for i in range(0, 5):\n",
        "    prediction = model(some_input)\n",
        "    loss = (ideal_output - prediction).pow(2).sum()\n",
        "    loss.backward()\n",
        "\n",
        "print(model.layer2.weight.grad[0][0:10])\n",
        "\n",
        "optimizer.zero_grad(set_to_none=False)\n",
        "\n",
        "print(model.layer2.weight.grad[0][0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BzXlGgpsWM8"
      },
      "source": [
        "After running the cell above, you should see that after running\n",
        "`loss.backward()` multiple times, the magnitudes of most of the\n",
        "gradients will be much larger. Failing to zero the gradients before\n",
        "running your next training batch will cause the gradients to blow up in\n",
        "this manner, causing incorrect and unpredictable learning results.\n",
        "\n",
        "Turning Autograd Off and On\n",
        "===========================\n",
        "\n",
        "There are situations where you will need fine-grained control over\n",
        "whether autograd is enabled. There are multiple ways to do this,\n",
        "depending on the situation.\n",
        "\n",
        "The simplest is to change the `requires_grad` flag on a tensor directly:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_xuTxeFzsWM8",
        "outputId": "aceae469-3ff6-4773-b1ed-ced989ed8f7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]], requires_grad=True)\n",
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.]], grad_fn=<MulBackward0>)\n",
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.ones(2, 3, requires_grad=True)\n",
        "print(a)\n",
        "\n",
        "b1 = 2 * a\n",
        "print(b1)\n",
        "\n",
        "a.requires_grad = False\n",
        "b2 = 2 * a\n",
        "print(b2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q66kMZyssWM8"
      },
      "source": [
        "In the cell above, we see that `b1` has a `grad_fn` (i.e., a traced\n",
        "computation history), which is what we expect, since it was derived from\n",
        "a tensor, `a`, that had autograd turned on. When we turn off autograd\n",
        "explicitly with `a.requires_grad = False`, computation history is no\n",
        "longer tracked, as we see when we compute `b2`.\n",
        "\n",
        "If you only need autograd turned off temporarily, a better way is to use\n",
        "the `torch.no_grad()`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TjiMsRFcsWM8",
        "outputId": "25e26d15-c6d0-468b-e4a8-e7051f489006",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]])\n",
            "tensor([[6., 6., 6.],\n",
            "        [6., 6., 6.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "a = torch.ones(2, 3, requires_grad=True) * 2\n",
        "b = torch.ones(2, 3, requires_grad=True) * 3\n",
        "\n",
        "c1 = a + b\n",
        "print(c1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    c2 = a + b\n",
        "\n",
        "print(c2)\n",
        "\n",
        "c3 = a * b\n",
        "print(c3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkganj0JsWM8"
      },
      "source": [
        "`torch.no_grad()` can also be used as a function or method decorator:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3UXYdR0nsWM9",
        "outputId": "1c1edd29-57e4-4225-caa9-313a590a0d56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
            "tensor([[5., 5., 5.],\n",
            "        [5., 5., 5.]])\n"
          ]
        }
      ],
      "source": [
        "def add_tensors1(x, y):\n",
        "    return x + y\n",
        "\n",
        "@torch.no_grad()\n",
        "def add_tensors2(x, y):\n",
        "    return x + y\n",
        "\n",
        "\n",
        "a = torch.ones(2, 3, requires_grad=True) * 2\n",
        "b = torch.ones(2, 3, requires_grad=True) * 3\n",
        "\n",
        "c1 = add_tensors1(a, b)\n",
        "print(c1)\n",
        "\n",
        "c2 = add_tensors2(a, b)\n",
        "print(c2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bweq-FuMsWM9"
      },
      "source": [
        "There's a corresponding context manager, `torch.enable_grad()`, for\n",
        "turning autograd on when it isn't already. It may also be used as a\n",
        "decorator.\n",
        "\n",
        "Finally, you may have a tensor that requires gradient tracking, but you\n",
        "want a copy that does not. For this we have the `Tensor` object's\n",
        "`detach()` method - it creates a copy of the tensor that is *detached*\n",
        "from the computation history:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HlKbEQYosWNB",
        "outputId": "1f3a0b6e-3031-462f-a22d-165d9ad9cfb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6985, 0.6296, 0.0938, 0.8055, 0.4826], requires_grad=True)\n",
            "tensor([0.6985, 0.6296, 0.0938, 0.8055, 0.4826])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(5, requires_grad=True)\n",
        "y = x.detach()\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE9008aZsWNB"
      },
      "source": [
        "We did this above when we wanted to graph some of our tensors. This is\n",
        "because `matplotlib` expects a NumPy array as input, and the implicit\n",
        "conversion from a PyTorch tensor to a NumPy array is not enabled for\n",
        "tensors with requires\\_grad=True. Making a detached copy lets us move\n",
        "forward.\n",
        "\n",
        "Autograd and In-place Operations\n",
        "================================\n",
        "\n",
        "In every example in this notebook so far, we've used variables to\n",
        "capture the intermediate values of a computation. Autograd needs these\n",
        "intermediate values to perform gradient computations. *For this reason,\n",
        "you must be careful about using in-place operations when using\n",
        "autograd.* Doing so can destroy information you need to compute\n",
        "derivatives in the `backward()` call. PyTorch will even stop you if you\n",
        "attempt an in-place operation on leaf variable that requires autograd,\n",
        "as shown below.\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>The following code cell throws a runtime error. This is expected.<pre><code>a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
        "torch.sin_(a)</code></pre></p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF96KYqQsWNB"
      },
      "source": [
        "Autograd Profiler\n",
        "=================\n",
        "\n",
        "Autograd tracks every step of your computation in detail. Such a\n",
        "computation history, combined with timing information, would make a\n",
        "handy profiler - and autograd has that feature baked in. Here's a quick\n",
        "example usage:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0cih3JuRsWNB",
        "outputId": "e3835bfa-4889-4844-e9a7-726bdbbad96c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "-------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "    aten::div        53.28%       3.964ms        53.28%       3.964ms       3.964us          1000  \n",
            "    aten::mul        46.72%       3.475ms        46.72%       3.475ms       3.475us          1000  \n",
            "-------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 7.438ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cpu')\n",
        "run_on_gpu = False\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    run_on_gpu = True\n",
        "\n",
        "x = torch.randn(2, 3, requires_grad=True)\n",
        "y = torch.rand(2, 3, requires_grad=True)\n",
        "z = torch.ones(2, 3, requires_grad=True)\n",
        "\n",
        "with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
        "    for _ in range(1000):\n",
        "        z = (z / x) * y\n",
        "\n",
        "print(prf.key_averages().table(sort_by='self_cpu_time_total'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **解释profile**\n",
        "这段代码是 PyTorch 中用于**性能分析（Profiling）的核心工具。它的主要作用是测量代码块中每个 PyTorch 算子（Operator）的执行时间**，帮助你找出程序的性能瓶颈。\n",
        "\n",
        "下面我把这几行代码拆解开来，详细解释每一部分的含义和作用：\n",
        "\n",
        "### 1. 开启性能分析器 (The Profiler Context)\n",
        "\n",
        "```python\n",
        "with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
        "\n",
        "```\n",
        "\n",
        "* **`with ... as ...:`**: 这是一个 Python 的**上下文管理器（Context Manager）**。\n",
        "* 当你进入这个 `with` 代码块时，PyTorch 的性能分析器开始工作（就像按下了秒表的“开始”）。\n",
        "* 当你退出这个代码块时，分析器停止工作（按下“停止”）。\n",
        "* 所有的分析数据（比如每个算子跑了多久、占用了多少内存）都会被保存在变量 `prf` 中。\n",
        "\n",
        "\n",
        "* **`torch.autograd.profiler.profile(...)`**: 这是旧版 PyTorch 的分析器接口（新版推荐使用 `torch.profiler`，但这个依然有效）。\n",
        "* **参数 `use_cuda=run_on_gpu**`:\n",
        "* 这是一个布尔值开关。\n",
        "* **如果为 `True**`: 分析器不仅记录 CPU 的时间，还会通过 CUDA 事件记录 GPU 内核的执行时间。这对于分析 GPU 训练的模型至关重要，因为 CPU 下发指令和 GPU 执行指令是异步的。\n",
        "* **如果为 `False**`: 只记录 CPU 端的执行时间。\n",
        "\n",
        "\n",
        "\n",
        "### 2. 被分析的代码块 (The Workload)\n",
        "\n",
        "```python\n",
        "    for _ in range(1000):\n",
        "        z = (z / x) * y\n",
        "\n",
        "```\n",
        "\n",
        "* 这是你想要“体检”的具体代码。\n",
        "* **发生了什么？** 在这 1000 次循环中，PyTorch 执行了除法（`div`）和乘法（`mul`）操作。\n",
        "* 由于你的张量（`x`, `y`, `z`）都设置了 `requires_grad=True`，PyTorch 在后台不仅是在做计算，还在**构建计算图（Computational Graph）**以便后续反向传播。Profiler 会记录下这些操作的开销。\n",
        "\n",
        "### 3. 生成并打印分析报告 (The Report)\n",
        "\n",
        "```python\n",
        "print(prf.key_averages().table(sort_by='self_cpu_time_total'))\n",
        "\n",
        "```\n",
        "\n",
        "这行代码负责把 `prf` 里记录的杂乱原始数据整理成一张人类可读的表格。\n",
        "\n",
        "* **`prf.key_averages()`**: **关键步骤——聚合数据**。\n",
        "* 原始数据是流水账：循环 1000 次，就会记录 1000 次除法和 1000 次乘法，总共 2000 多条记录。\n",
        "* `key_averages()` 会把**相同名称的算子合并在一起**。比如把 1000 次 `div` 操作的时间加起来，算出一个平均值和总值。如果不加这一步，打印出来的表格会有几千行，没法看。\n",
        "\n",
        "\n",
        "* **`.table(...)`**: 将聚合后的数据格式化为一个字符串表格。\n",
        "* **`sort_by='self_cpu_time_total'`**: **决定排序方式**。\n",
        "* 这告诉表格按照“**自身 CPU 总时间**”从大到小排序。排在最上面的就是最耗时的操作（也就是性能瓶颈）。\n",
        "* **常见的排序键值（Sort Keys）解释**：\n",
        "* **`self_cpu_time_total`**: 该算子自身逻辑消耗的 CPU 总时间（不包含它调用的子算子的时间）。**找瓶颈通常看这个。**\n",
        "* `cpu_time_total`: 该算子加上它调用的所有子算子消耗的总时间。\n",
        "* `cuda_time_total`: GPU 上的总执行时间（如果 `use_cuda=True`）。\n",
        "* `cpu_memory_usage`: CPU 内存使用量。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 预期输出结果示例\n",
        "\n",
        "当你运行这段代码时，控制台会打印出类似下面的表格：\n",
        "\n",
        "```text\n",
        "-------------------------  ------------  ------------  ------------  ------------  ------------\n",
        "                     Name      Self CPU     CPU total  CPU time avg     Self CUDA    CUDA total\n",
        "-------------------------  ------------  ------------  ------------  ------------  ------------\n",
        "                aten::div       5.20 ms       5.20 ms       5.20 us       0.00 us       0.00 us\n",
        "                aten::mul       4.80 ms       4.80 ms       4.80 us       0.00 us       0.00 us\n",
        "-------------------------  ------------  ------------  ------------  ------------  ------------\n",
        "Self CPU time total: 10.00 ms\n",
        "\n",
        "```\n",
        "\n",
        "**表格解读：**\n",
        "\n",
        "1. **Name**: 算子名称（如 `aten::div` 是除法，`aten::mul` 是乘法）。\n",
        "2. **Self CPU**: 这些算子自身在 CPU 上跑了多久（聚合后的总时间）。\n",
        "3. **CPU time avg**: 平均每次调用跑了多久（例如 `5.20 ms / 1000 = 5.20 us`）。\n",
        "\n",
        "### 总结\n",
        "\n",
        "这段代码的作用是：**给中间的 1000 次循环做个“体检”，统计里面用到的除法和乘法操作到底消耗了多少 CPU 时间，并按照耗时从高到低打印出来，让你一眼看出谁是性能拖油瓶。**"
      ],
      "metadata": {
        "id": "6mfMVfL6K3fI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLmhrrLUsWNB"
      },
      "source": [
        "The profiler can also label individual sub-blocks of code, break out the\n",
        "data by input tensor shape, and export data as a Chrome tracing tools\n",
        "file. For full details of the API, see the\n",
        "[documentation](https://pytorch.org/docs/stable/autograd.html#profiler).\n",
        "\n",
        "Advanced Topic: More Autograd Detail and the High-Level API\n",
        "===========================================================\n",
        "\n",
        "If you have a function with an n-dimensional input and m-dimensional\n",
        "output, $\\vec{y}=f(\\vec{x})$, the complete gradient is a matrix of the\n",
        "derivative of every output with respect to every input, called the\n",
        "*Jacobian:*\n",
        "\n",
        "$$\\begin{aligned}\n",
        "J\n",
        "=\n",
        "\\left(\\begin{array}{ccc}\n",
        "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "\\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "\\end{array}\\right)\n",
        "\\end{aligned}$$\n",
        "\n",
        "If you have a second function, $l=g\\left(\\vec{y}\\right)$ that takes\n",
        "m-dimensional input (that is, the same dimensionality as the output\n",
        "above), and returns a scalar output, you can express its gradients with\n",
        "respect to $\\vec{y}$ as a column vector,\n",
        "$v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$\n",
        "- which is really just a one-column Jacobian.\n",
        "\n",
        "More concretely, imagine the first function as your PyTorch model (with\n",
        "potentially many inputs and many outputs) and the second function as a\n",
        "loss function (with the model's output as input, and the loss value as\n",
        "the scalar output).\n",
        "\n",
        "If we multiply the first function's Jacobian by the gradient of the\n",
        "second function, and apply the chain rule, we get:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
        "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "\\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "\\end{array}\\right)\\left(\\begin{array}{c}\n",
        "\\frac{\\partial l}{\\partial y_{1}}\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{\\partial l}{\\partial y_{m}}\n",
        "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
        "\\frac{\\partial l}{\\partial x_{1}}\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{\\partial l}{\\partial x_{n}}\n",
        "\\end{array}\\right)\n",
        "\\end{aligned}$$\n",
        "\n",
        "Note: You could also use the equivalent operation $v^{T}\\cdot J$, and\n",
        "get back a row vector.\n",
        "\n",
        "The resulting column vector is the *gradient of the second function with\n",
        "respect to the inputs of the first* - or in the case of our model and\n",
        "loss function, the gradient of the loss with respect to the model\n",
        "inputs.\n",
        "\n",
        "**\\`\\`torch.autograd\\`\\` is an engine for computing these products.**\n",
        "This is how we accumulate the gradients over the learning weights during\n",
        "the backward pass.\n",
        "\n",
        "For this reason, the `backward()` call can *also* take an optional\n",
        "vector input. This vector represents a set of gradients over the tensor,\n",
        "which are multiplied by the Jacobian of the autograd-traced tensor that\n",
        "precedes it. Let's try a specific example with a small vector:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49Q2oYEzsWNB"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "while y.data.norm() < 1000:\n",
        "    y = y * 2\n",
        "\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U9VB3XPsWNB"
      },
      "source": [
        "If we tried to call `y.backward()` now, we'd get a runtime error and a\n",
        "message that gradients can only be *implicitly* computed for scalar\n",
        "outputs. For a multi-dimensional output, autograd expects us to provide\n",
        "gradients for those three outputs that it can multiply into the\n",
        "Jacobian:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9YcmGmDsWNB"
      },
      "outputs": [],
      "source": [
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) # stand-in for gradients\n",
        "y.backward(v)\n",
        "\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPpLe-FBsWNC"
      },
      "source": [
        "(Note that the output gradients are all related to powers of two - which\n",
        "we'd expect from a repeated doubling operation.)\n",
        "\n",
        "The High-Level API\n",
        "==================\n",
        "\n",
        "There is an API on autograd that gives you direct access to important\n",
        "differential matrix and vector operations. In particular, it allows you\n",
        "to calculate the Jacobian and the *Hessian* matrices of a particular\n",
        "function for particular inputs. (The Hessian is like the Jacobian, but\n",
        "expresses all partial *second* derivatives.) It also provides methods\n",
        "for taking vector products with these matrices.\n",
        "\n",
        "Let's take the Jacobian of a simple function, evaluated for a 2\n",
        "single-element inputs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVXDH0UAsWNC"
      },
      "outputs": [],
      "source": [
        "def exp_adder(x, y):\n",
        "    return 2 * x.exp() + 3 * y\n",
        "\n",
        "inputs = (torch.rand(1), torch.rand(1)) # arguments for the function\n",
        "print(inputs)\n",
        "torch.autograd.functional.jacobian(exp_adder, inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OP1pYKNsWNC"
      },
      "source": [
        "If you look closely, the first output should equal $2e^x$ (since the\n",
        "derivative of $e^x$ is $e^x$), and the second value should be 3.\n",
        "\n",
        "You can, of course, do this with higher-order tensors:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAu_KLdFsWNC"
      },
      "outputs": [],
      "source": [
        "inputs = (torch.rand(3), torch.rand(3)) # arguments for the function\n",
        "print(inputs)\n",
        "torch.autograd.functional.jacobian(exp_adder, inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRGfhszfsWNC"
      },
      "source": [
        "The `torch.autograd.functional.hessian()` method works identically\n",
        "(assuming your function is twice differentiable), but returns a matrix\n",
        "of all second derivatives.\n",
        "\n",
        "There is also a function to directly compute the vector-Jacobian\n",
        "product, if you provide the vector:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG88qX_usWNC"
      },
      "outputs": [],
      "source": [
        "def do_some_doubling(x):\n",
        "    y = x * 2\n",
        "    while y.data.norm() < 1000:\n",
        "        y = y * 2\n",
        "    return y\n",
        "\n",
        "inputs = torch.randn(3)\n",
        "my_gradients = torch.tensor([0.1, 1.0, 0.0001])\n",
        "torch.autograd.functional.vjp(do_some_doubling, inputs, v=my_gradients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjiuJtbrsWNC"
      },
      "source": [
        "The `torch.autograd.functional.jvp()` method performs the same matrix\n",
        "multiplication as `vjp()` with the operands reversed. The `vhp()` and\n",
        "`hvp()` methods do the same for a vector-Hessian product.\n",
        "\n",
        "For more information, including performance notes on the [docs for the\n",
        "functional\n",
        "API](https://pytorch.org/docs/stable/autograd.html#functional-higher-level-api)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}