{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaredmccain-ux/pytorch-tutorial-learning-notes/blob/main/_downloads/autogradqs_tutorial/autogradqs_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNrCMfJgZMuu"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://docs.pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICigD5fmZMuv"
      },
      "source": [
        "[Learn the Basics](intro.html) \\|\\|\n",
        "[Quickstart](quickstart_tutorial.html) \\|\\|\n",
        "[Tensors](tensorqs_tutorial.html) \\|\\| [Datasets &\n",
        "DataLoaders](data_tutorial.html) \\|\\|\n",
        "[Transforms](transforms_tutorial.html) \\|\\| [Build\n",
        "Model](buildmodel_tutorial.html) \\|\\| **Autograd** \\|\\|\n",
        "[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n",
        "Model](saveloadrun_tutorial.html)\n",
        "\n",
        "Automatic Differentiation with `torch.autograd`\n",
        "===============================================\n",
        "\n",
        "When training neural networks, the most frequently used algorithm is\n",
        "**back propagation**. In this algorithm, parameters (model weights) are\n",
        "adjusted according to the **gradient** of the loss function with respect\n",
        "to the given parameter.\n",
        "\n",
        "To compute those gradients, PyTorch has a built-in differentiation\n",
        "engine called `torch.autograd`. It supports automatic computation of\n",
        "gradient for any computational graph.\n",
        "\n",
        "Consider the simplest one-layer neural network, with input `x`,\n",
        "parameters `w` and `b`, and some loss function. It can be defined in\n",
        "PyTorch in the following manner:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QvetdzwvZMux"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w)+b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5YNuqzxZMux"
      },
      "source": [
        "Tensors, Functions and Computational graph\n",
        "==========================================\n",
        "\n",
        "This code defines the following **computational graph**:\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/basics/comp-graph.png)\n",
        "\n",
        "In this network, `w` and `b` are **parameters**, which we need to\n",
        "optimize. Thus, we need to be able to compute the gradients of loss\n",
        "function with respect to those variables. In order to do that, we set\n",
        "the `requires_grad` property of those tensors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBZhKGtuZMux"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>You can set the value of <code>requires_grad</code> when creating atensor, or later by using <code>x.requires_grad_(True)</code> method.</p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvS2GTQ-ZMuy"
      },
      "source": [
        "A function that we apply to tensors to construct computational graph is\n",
        "in fact an object of class `Function`. This object knows how to compute\n",
        "the function in the *forward* direction, and also how to compute its\n",
        "derivative during the *backward propagation* step. **A reference to the\n",
        "backward propagation function is stored in `grad_fn` property of a\n",
        "tensor. You can find more information of `Function` [in the\n",
        "documentation]**(https://pytorch.org/docs/stable/autograd.html#function).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l_gnnWj1ZMuy",
        "outputId": "76ef2906-80b1-4eae-9600-5ef264dfd82b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x7be105c2f2e0>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7be129fef6a0>\n"
          ]
        }
      ],
      "source": [
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dteYgv1xZMuz"
      },
      "source": [
        "Computing Gradients\n",
        "===================\n",
        "\n",
        "To optimize weights of parameters in the neural network, we need to\n",
        "compute the derivatives of our loss function with respect to parameters,\n",
        "namely, we need $\\frac{\\partial loss}{\\partial w}$ and\n",
        "$\\frac{\\partial loss}{\\partial b}$ under some fixed values of `x` and\n",
        "`y`. To compute those derivatives, we call `loss.backward()`, and then\n",
        "retrieve the values from `w.grad` and `b.grad`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z8zDEZH5ZMuz",
        "outputId": "e8105e3c-0b27-4ab9-ea9f-7cbb021277d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0019, 0.0292, 0.3007],\n",
            "        [0.0019, 0.0292, 0.3007],\n",
            "        [0.0019, 0.0292, 0.3007],\n",
            "        [0.0019, 0.0292, 0.3007],\n",
            "        [0.0019, 0.0292, 0.3007]])\n",
            "tensor([0.0019, 0.0292, 0.3007])\n"
          ]
        }
      ],
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I4ZxjA9ZMu0"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<ul>\n",
        "<li>We can only obtain the <code>grad</code> properties for the leafnodes of the computational graph, which have <code>requires_grad</code> propertyset to <code>True</code>. For all other nodes in our graph, gradients will not beavailable.- We can only perform gradient calculations using<code>backward</code> once on a given graph, for performance reasons. If we needto do several <code>backward</code> calls on the same graph, we need to pass<code>retain_graph=True</code> to the <code>backward</code> call.</li>\n",
        "</ul>\n",
        "```\n",
        "\n",
        "</div>\n",
        "\n",
        "# **解释上面这句话：**\n",
        "简单来说，**多次反向传播**指的是在同一个计算图（Computational Graph）上执行超过一次的 `.backward()` 操作。\n",
        "\n",
        "在 PyTorch 的默认机制中，计算图是**一次性**的。为了节省内存，一旦你调用了 `loss.backward()`，PyTorch 就会在完成梯度计算后立即销毁这个计算图。如果你尝试第二次对同一个变量调用 `.backward()`，程序会报错。\n",
        "\n",
        "---\n",
        "\n",
        "### 为什么要进行“多次”反向传播？\n",
        "\n",
        "虽然大多数基础模型（如简单的分类器）只需要一个 Loss 进行一次反向传播，但在以下复杂的深度学习场景中，多次反向传播是必须的：\n",
        "\n",
        "#### 1. 多个损失函数 (Multiple Losses)\n",
        "\n",
        "假设你的模型有两个输出任务，分别计算出  和 。这两个 Loss 共享了模型的前半部分权重。\n",
        "\n",
        "* 如果你先执行 `Loss_1.backward()`，计算图会被销毁。\n",
        "* 当你接着执行 `Loss_2.backward()` 时，程序会因为找不到计算图而崩溃。\n",
        "* **解决方法：** 执行 `Loss_1.backward(retain_graph=True)`，这样图会被保留，供  使用。\n",
        "\n",
        "#### 2. 生成对抗网络 (GANs)\n",
        "\n",
        "在训练 GAN 时，D（判别器）和 G（生成器）的损失函数往往高度耦合。有时候你需要先计算判别器的梯度，保留图，再计算生成器的梯度。\n",
        "\n",
        "#### 3. 梯度累积 (Gradient Accumulation) 的特殊变体\n",
        "\n",
        "虽然常见的梯度累积（多个 Batch 后才 `step`）不需要 `retain_graph`，但在某些复杂的强化学习算法（如 A3C）或二阶导数计算中，需要对同一个图进行多次路径回溯。\n",
        "\n",
        "---\n",
        "\n",
        "### 形象的类比：过河拆桥\n",
        "\n",
        "你可以把**计算图**想象成一座为了过河（计算梯度）而临时搭建的**桥**：\n",
        "\n",
        "* **默认状态：** 你过完桥（完成 `backward`）后，为了节省建筑材料（内存），PyTorch 会立刻把桥拆掉。\n",
        "* **多次反向传播：** 如果你告诉 PyTorch `retain_graph=True`，就相当于说：“先别拆桥，我一会儿还要再跑回来一趟。”\n",
        "\n",
        "---\n",
        "\n",
        "### 代码对比\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "y = x * 2\n",
        "loss = y.sum()\n",
        "\n",
        "# 第一次反向传播\n",
        "loss.backward(retain_graph=True)\n",
        "print(x.grad) # 正常输出\n",
        "\n",
        "# 如果没有 retain_graph=True，下面这行会报错\n",
        "loss.backward()\n",
        "print(x.grad) # 梯度会累加\n",
        "\n",
        "```\n",
        "\n",
        "### 总结\n",
        "\n",
        "* **单次反向传播（默认）：** 节省内存，图用完即删。\n",
        "* **多次反向传播（`retain_graph=True`）：** 允许你多次利用同一组中间变量计算梯度，常用于多任务学习或复杂的生成模型。\n",
        "\n",
        "\n",
        "# **解释迭代和多次反向传播**\n",
        "这是一个非常经典的困惑点。要解开这个疑问，最关键的是区分两个概念：**“训练迭代（Training Iterations）”** 和 **“单次计算图内的多次反向传播（Multiple Backwards on the same Graph）”**。\n",
        "\n",
        "你平时理解的“更新很多次参数”，其实是**每一轮迭代都用了全新的计算图**。\n",
        "\n",
        "---\n",
        "\n",
        "### 1. 核心区别：新图 vs. 旧图\n",
        "\n",
        "在 PyTorch 这种**动态计算图**框架中，流程是这样的：\n",
        "\n",
        "* **标准的训练循环（每一轮迭代）：**\n",
        "1. **Forward:** 你跑一次 `output = model(input)`。此时，PyTorch 会在内存中**从无到有现场搭建**一座“桥”（计算图），记录下所有的中间变量和运算关系。\n",
        "2. **Backward:** 你跑一次 `loss.backward()`。PyTorch 顺着这座桥走回去，计算出梯度。\n",
        "3. **桥被拆除：** 为了节省内存，梯度算完的一瞬间，这张图（以及它占用的中间变量内存）会被**立即销毁**。\n",
        "4. **下一轮迭代：** 当你下一次跑 `model(input)` 时，它会重新搭建一张**全新的桥**。\n",
        "\n",
        "\n",
        "* **文档里说的“多次反向传播”：**\n",
        "是指在**同一个 Forward 产生的同一张图**上，连续调用两次 `.backward()`。\n",
        "> **类比：** 你建了一座桥，走过去之后（第一次 backward），如果你想不拆桥再走一遍（第二次 backward），你就必须声明 `retain_graph=True`。\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 2. 为什么“每一轮迭代重新建图”不麻烦？\n",
        "\n",
        "你可能会觉得：每轮都重建一张图，这不是很浪费性能吗？\n",
        "\n",
        "其实不然，这正是 PyTorch 灵活的核心所在：\n",
        "\n",
        "1. **内存效率：** 如果不销毁图，随着训练进行，你的显存（VRAM）瞬间就会被成千上万个中间层的激活值（Activations）挤爆。\n",
        "2. **动态性：** 因为每轮都重绘，你可以让每一轮的神经网络结构都不一样（比如在循环中改变层的数量，或者根据条件跳转路径），这在处理变长序列（NLP）或动态结构时非常强大。\n",
        "\n",
        "---\n",
        "\n",
        "### 3. 什么时候真的需要“多次反向传播”？\n",
        "\n",
        "既然大部分时间我们都是“用完就拆”，那什么变态场景需要 `retain_graph=True` 呢？\n",
        "\n",
        "* **多任务学习（Multi-task Learning）：**\n",
        "如果你有两个独立的 Loss（例如  负责分类， 负责回归），它们共享同一个骨干网络。\n",
        "```python\n",
        "# 错误写法：第一行执行完图就没了，第二行会报错\n",
        "loss1.backward()\n",
        "loss2.backward()\n",
        "\n",
        "# 正确写法：\n",
        "loss1.backward(retain_graph=True) # 保留图给下一个人用\n",
        "loss2.backward() # 这一次用完可以拆了\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "* **对抗训练（GANs）：**\n",
        "当生成器和判别器的损失函数需要交替利用同一个前向计算结果来更新梯度时。\n",
        "\n",
        "---\n",
        "\n",
        "### 总结\n",
        "\n",
        "| 场景 | 是否涉及“多次反向传播” | 是否需要 `retain_graph` |\n",
        "| --- | --- | --- |\n",
        "| **正常训练（100 个 Epoch）** | 否（每轮都是独立的新图） | 不需要 |\n",
        "| **同一个 Batch 里计算多个 Loss** | **是**（共享同一张图） | **需要** |\n",
        "| **二阶导数计算** | **是**（需要保留一阶导数的图） | **需要** |\n",
        "\n",
        "所以，这并不会让你平时的训练变得不方便。相反，PyTorch 默认“用完即焚”的做法，正是为了让你能在有限的显存里跑更大的模型。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI4c5KjNZMu0"
      },
      "source": [
        "Disabling Gradient Tracking\n",
        "===========================\n",
        "\n",
        "By default, all tensors with `requires_grad=True` are tracking their\n",
        "computational history and support gradient computation. However, there\n",
        "are some cases when we do not need to do that, for example, when we have\n",
        "trained the model and just want to apply it to some input data, i.e. we\n",
        "only want to do *forward* computations through the network. We can stop\n",
        "tracking computations by surrounding our computation code with\n",
        "`torch.no_grad()` block:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xvkCIIRTZMu0",
        "outputId": "7f9bab4d-1c2f-470e-8e9a-730f4a10cbdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwzun5tbZMu0"
      },
      "source": [
        "Another way to achieve the same result is to use the `detach()` method\n",
        "on the tensor:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0tWt6MWaZMu0",
        "outputId": "a727b946-e706-4691-e2ce-75361cdec321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvVMJxkXZMu1"
      },
      "source": [
        "There are reasons you might want to disable gradient tracking:\n",
        "\n",
        ":   -   To mark some parameters in your neural network as **frozen\n",
        "        parameters**.\n",
        "    -   To **speed up computations** when you are only doing forward\n",
        "        pass, because computations on tensors that do not track\n",
        "        gradients would be more efficient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJXzULtCZMu1"
      },
      "source": [
        "More on Computational Graphs\n",
        "============================\n",
        "\n",
        "Conceptually, autograd keeps a record of data (tensors) and all executed\n",
        "operations (along with the resulting new tensors) in a directed acyclic\n",
        "graph (DAG) consisting of\n",
        "[Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
        "objects. In this DAG, leaves are the input tensors, roots are the output\n",
        "tensors. By tracing this graph from roots to leaves, you can\n",
        "automatically compute the gradients using the chain rule.\n",
        "\n",
        "In a forward pass, autograd does two things simultaneously:\n",
        "\n",
        "-   run the requested operation to compute a resulting tensor\n",
        "-   maintain the operation's *gradient function* in the DAG.\n",
        "\n",
        "The backward pass kicks off when `.backward()` is called on the DAG\n",
        "root. `autograd` then:\n",
        "\n",
        "-   computes the gradients from each `.grad_fn`,\n",
        "-   accumulates them in the respective tensor's `.grad` attribute\n",
        "-   using the chain rule, propagates all the way to the leaf tensors.\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>An important thing to note is that the graph is recreated from scratch; after each<code>.backward()</code> call, autograd starts populating a new graph. This isexactly what allows you to use control flow statements in your model;you can change the shape, size and operations at every iteration ifneeded.</p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFJ-3sJ6ZMu1"
      },
      "source": [
        "Optional Reading: Tensor Gradients and Jacobian Products\n",
        "========================================================\n",
        "\n",
        "In many cases, we have a scalar loss function, and we need to compute\n",
        "the gradient with respect to some parameters. However, there are cases\n",
        "when the output function is an arbitrary tensor. In this case, PyTorch\n",
        "allows you to compute so-called **Jacobian product**, and not the actual\n",
        "gradient.\n",
        "\n",
        "For a vector function $\\vec{y}=f(\\vec{x})$, where\n",
        "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ and\n",
        "$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$, a gradient of $\\vec{y}$ with\n",
        "respect to $\\vec{x}$ is given by **Jacobian matrix**:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "J=\\left(\\begin{array}{ccc}\n",
        "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        "   \\vdots & \\ddots & \\vdots\\\\\n",
        "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "   \\end{array}\\right)\n",
        "\\end{aligned}$$\n",
        "\n",
        "Instead of computing the Jacobian matrix itself, PyTorch allows you to\n",
        "compute **Jacobian Product** $v^T\\cdot J$ for a given input vector\n",
        "$v=(v_1 \\dots v_m)$. This is achieved by calling `backward` with $v$ as\n",
        "an argument. The size of $v$ should be the same as the size of the\n",
        "original tensor, with respect to which we want to compute the product:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twwqI6O0ZMu1"
      },
      "outputs": [],
      "source": [
        "inp = torch.eye(4, 5, requires_grad=True)\n",
        "out = (inp+1).pow(2).t()\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"First call\\n{inp.grad}\")\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"\\nSecond call\\n{inp.grad}\")\n",
        "inp.grad.zero_()\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vALmZPcPZMu1"
      },
      "source": [
        "Notice that when we call `backward` for the second time with the same\n",
        "argument, the value of the gradient is different. This happens because\n",
        "when doing `backward` propagation, PyTorch **accumulates the\n",
        "gradients**, i.e. the value of computed gradients is added to the `grad`\n",
        "property of all leaf nodes of computational graph. If you want to\n",
        "compute the proper gradients, you need to zero out the `grad` property\n",
        "before. In real-life training an *optimizer* helps us to do this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5b0KaCrZMu1"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>Previously we were calling <code>backward()</code> function withoutparameters. This is essentially equivalent to calling<code>backward(torch.tensor(1.0))</code>, which is a useful way to compute thegradients in case of a scalar-valued function, such as loss duringneural network training.</p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ9QxLhFZMu2"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_MIk4NJZMu2"
      },
      "source": [
        "Further Reading\n",
        "===============\n",
        "\n",
        "-   [Autograd\n",
        "    Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}